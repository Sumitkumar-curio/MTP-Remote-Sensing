# -*- coding: utf-8 -*-
"""QuantumCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uZKEJy9ZD4g2O-xiImA0jPL67PX1lF-S
"""

!unzip /content/EuroSAT.zip

!pip install qiskit==1.0.0
!pip install pylatexenc

!pip install qiskit_ibm_runtime

from qiskit_ibm_runtime import QiskitRuntimeService

# Set your token from IBM Quantum (replace 'TOKEN' with your actual IBM Quantum token)
token = 'deed0480b9f0aa55dcba0e6b87e880978deecf31964257d1b986d4968f4e4b980ef0b38c131d93bdde5f8b1648b48a986db7e6f02f3d15dd7f1289b7f147e2ba'

# Authenticate using the provided token and channel
service = QiskitRuntimeService(token=token, channel='ibm_quantum')

# List available backends (e.g., IBM Q Experience)
backends = service.backends()  # Get available backends

# Select the first backend from the list
backend = backends[0]

# Print backend information
print(f"Backend: {backend}")

# Commented out IPython magic to ensure Python compatibility.


import qiskit
import numpy as np
from tqdm import tqdm
from matplotlib import pyplot as plt
# %matplotlib inline
import torch
from torch.autograd import Function
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F


if torch.cuda.is_available():
  device = torch.device("cuda:0")
  print("Running on the GPU")
else:
  device = torch.device("cpu")
  print("Running on the CPU")

from qiskit_ibm_runtime import QiskitRuntimeService
from qiskit import QuantumCircuit
from qiskit import QuantumCircuit, transpile
import numpy as np

# Set your token from IBM Quantum (replace 'TOKEN' with your actual IBM Quantum token)
token = 'deed0480b9f0aa55dcba0e6b87e880978deecf31964257d1b986d4968f4e4b980ef0b38c131d93bdde5f8b1648b48a986db7e6f02f3d15dd7f1289b7f147e2ba'

# Authenticate using the provided token and specify the channel
service = QiskitRuntimeService(token=token, channel='ibm_quantum') # You can use 'ibm-quantum' or 'ibm-cloud'
# List available backends directly using the QiskitRuntimeService
backends = service.backends()
print(f"Available backends: {backends}")

if backends:
    backend = backends[0]  # Select the first available backend
    print(f"Selected backend: {backend}")

    # Create a quantum circuit to run on the backend
    qc = QuantumCircuit(2, 2)  # 2 qubits and 2 classical bits
    qc.h(0)  # Apply Hadamard gate to qubit 0
    qc.cx(0, 1)  # Apply CNOT gate between qubit 0 and qubit 1
    qc.measure([0, 1], [0, 1])  # Measure both qubits

    # Transpile the circuit to the selected backend
    transpiled_circuit = transpile(qc, backend)
    print("Transpiled circuit:", transpiled_circuit)

    # Submit the transpiled circuit to the selected backend
    job = backend.run(transpiled_circuit)  # Execute the job on the selected backend

    # Wait for the job to complete and get the results
    result = job.result()

    # Get the counts of the measurements
    counts = result.get_counts()
    print("Measurement results:", counts)
else:
    print("No backends available.")

!pip install qiskit-aer

# Commented out IPython magic to ensure Python compatibility.


from qiskit import QuantumRegister, ClassicalRegister, QuantumCircuit
from qiskit_aer import AerSimulator
import numpy as np
import qiskit
import numpy as np
from tqdm import tqdm
from matplotlib import pyplot as plt
# %matplotlib inline
import torch
from torch.autograd import Function
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F


if torch.cuda.is_available():
  device = torch.device("cuda:0")
  print("Running on the GPU")
else:
  device = torch.device("cpu")
  print("Running on the CPU")

from qiskit_ibm_runtime import QiskitRuntimeService
from qiskit.circuit import Parameter, QuantumCircuit
from qiskit_ibm_runtime import (
    EstimatorV2 as Estimator,
    QiskitRuntimeService,
)
from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager
from qiskit.quantum_info import Pauli, SparsePauliOp
import numpy as np

#np.random.seed = 314

NUM_QUBITS = 4
NUM_SHOTS = 800 #3000
SHIFT = np.pi/4
LEARNING_RATE = 0.0002
MOMENTUM = 0.5

SIMULATOR = AerSimulator()

# create list of all possible outputs of quantum circuit (2**NUM_QUBITS possible)
import itertools
def create_QC_OUTPUTS():
    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))
    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]

QC_OUTPUTS = create_QC_OUTPUTS()
print(QC_OUTPUTS)

class QiskitCircuit():

    def __init__(self, n_qubits, backend, shots):
        # --- Circuit definition ---
        self.circuit = qiskit.QuantumCircuit(n_qubits)
        self.n_qubits = n_qubits
        self.thetas = {k : Parameter('Theta'+str(k))for k in range(self.n_qubits)}

        self.circuit.h(0)
        self.circuit.cx(1, 0)
        self.circuit.cx(2, 1)
        self.circuit.cx(3, 2)

        self.circuit.barrier()

        for k in range(0, 4):
            self.circuit.ry(self.thetas[k], k)

        self.circuit.barrier()

        self.circuit.cx(3, 2)
        self.circuit.cx(2, 1)
        self.circuit.cx(1, 0)
        self.circuit.h(0)

        self.circuit.measure_all()
        # ---------------------------

        self.backend = backend
        self.shots = shots


    def N_qubit_expectation_Z(self,counts, shots, nr_qubits):
        expects = np.zeros(len(QC_OUTPUTS))
        for k in range(len(QC_OUTPUTS)):
            key = QC_OUTPUTS[k]
            perc = counts.get(key, 0)/shots
            expects[k] = perc
        return expects

    def run(self, i):
        params = i
        #print('params = {}'.format(len(params)))
        backend = Aer.get_backend('AerSimulator')

        job_sim = execute(self.circuit,
                              self.backend,
                              shots=self.shots,
                              parameter_binds = [{self.thetas[k] : params[k].item() for k in range(1*NUM_QUBITS)}])
#
        result_sim = job_sim.result()
        counts = result_sim.get_counts(self.circuit)
        return self.N_qubit_expectation_Z(counts, self.shots, NUM_QUBITS)

circuit = QiskitCircuit(NUM_QUBITS, SIMULATOR, NUM_SHOTS)
#print('Expected value for rotation [pi/4]: {}'.format(circuit.run(torch.Tensor([np.pi/4]*NUM_QUBITS))))
circuit.circuit.draw(output='mpl')#, filename='Figures/{}-qubit circuit ryN.jpg'.format(NUM_QUBITS)

class TorchCircuit(Function):

    @staticmethod
    def forward(ctx, i):
        if not hasattr(ctx, 'QiskitCirc'):
            ctx.QiskitCirc = QiskitCircuit(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)

        exp_value = ctx.QiskitCirc.run(i)

        result = torch.tensor([exp_value])

        ctx.save_for_backward(result, i)

        return result

    @staticmethod
    def backward(ctx, grad_output):

        forward_tensor, i = ctx.saved_tensors
#         print('forward_tensor = {}'.format(forward_tensor))
        input_numbers = i
#         print('input_numbers = {}'.format(input_numbers))
        gradients = torch.Tensor()

        for k in range(1*NUM_QUBITS):
            shift_right = input_numbers.detach().clone()
            shift_right[k] = shift_right[k] + SHIFT
            shift_left = input_numbers.detach().clone()
            shift_left[k] = shift_left[k] - SHIFT

#             print('shift_right = {}, shift_left = {}'.format(shift_right, shift_left))

            expectation_right = ctx.QiskitCirc.run(shift_right)
            expectation_left  = ctx.QiskitCirc.run(shift_left)
#             print('expectation_right = {}, \nexpectation_left = {}'.format(expectation_right, expectation_left))

            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])
            # rescale gradient
#             gradient = gradient / torch.norm(gradient)
#             print('gradient for k={}: {}'.format(k, gradient))
            gradients = torch.cat((gradients, gradient.float()))

        result = torch.Tensor(gradients)
#         print('gradients = {}'.format(result))
#         print('grad_output = {}'.format(grad_output))

        return (result.float() * grad_output.float()).T

import sys
sys.path.append('/content/EuroSAT')

from DatasetHandler import DatasetHandler
dataset_root = '/content/EuroSAT'
handler = DatasetHandler(dataset_root)

classes = []
for i, c in enumerate(handler.classes):
  cl = c.split('/')[-1]
  classes.append(cl)
classes.sort()
classes

imgs_path, imgs_label = handler.load_paths_labels(dataset_root, classes=classes)
print('Dataset images:', len(imgs_path), 'Dataset labels:', len(imgs_label))
print('Dataset sample ->', imgs_path[0], imgs_label[0])

train_imgs, train_labels, val_images, val_labels = handler.train_validation_split(imgs_path, imgs_label, split_factor=0.2)
print('X_train shape:', train_imgs.shape, 'Y_train shape:', train_labels.shape)
print('  X_val shape: ', val_images.shape, '  Y_val shape: ', val_labels.shape)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)

        #self.conv2_drop = nn.Dropout2d()

        self.fc1 = nn.Linear(2304, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 42)

        self.fc4 = nn.Linear(2304, 1*NUM_QUBITS)

        self.qc = TorchCircuit.apply

        self.fc5 = nn.Linear(16, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2(x), 2))
        x = F.relu(F.max_pool2d(self.conv3(x), 2))

        x = x.view(-1, 2304)
        #x = F.relu(self.fc1(x))
        #x = F.dropout(x, training=self.training)
        #x = F.relu(self.fc2(x))

        #x = F.relu(self.fc3(x))

        x = self.fc4(x)
        x = np.pi*torch.tanh(x)

        x = self.qc(x[0]) # QUANTUM LAYER

        x = F.relu(x)
        #print('output of QC = {}'.format(x))

#         # softmax rather than sigmoid
        x = self.fc5(x.float())
        #print('output of Linear(1, 2): {}'.format(x))
        x = F.softmax(x, 1)

        #x = torch.sigmoid(x)
        #x = torch.cat((x, 1-x), -1)
        return x


    def predict(self, x):
        # apply softmax
        pred = self.forward(x)
#         print(pred)
        ans = torch.argmax(pred[0]).item()
        return torch.tensor(ans)

network = Net()#.to(device)
optimizer = optim.Adam(network.parameters(), lr=0.0002)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

#from torchsummary import summary
#summary(network, (3, 64, 64))

train_loader = iter(handler.qcnn_data_loader(train_imgs, train_labels, batch_size = 1, img_shape = (64,64,3)))
test_loader = iter(handler.qcnn_data_loader(val_images, val_labels, batch_size = 1, img_shape = (64,64,3)))

checkpoint = torch.load('/content/model-bell-2.pt')
network.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

from qiskit_ibm_runtime import QiskitRuntimeService

# Replace with your actual token
token = 'deed0480b9f0aa55dcba0e6b87e880978deecf31964257d1b986d4968f4e4b980ef0b38c131d93bdde5f8b1648b48a986db7e6f02f3d15dd7f1289b7f147e2ba'

# Save your account
service = QiskitRuntimeService(token=token, channel='ibm_quantum')

print("Token saved successfully.")

from qiskit_aer import AerSimulator as Aer
backend  = AerSimulator()
train_loss_list = []
val_loss_list = []
epochs = 10
loss_func = nn.CrossEntropyLoss()

for epoch in range(epochs):
  train_loader = iter(handler.qcnn_data_loader(train_imgs, train_labels, batch_size = 1, img_shape = (64,64,3)))
  test_loader = iter(handler.qcnn_data_loader(val_images, val_labels, batch_size = 1, img_shape = (64,64,3)))
  total_loss = []
  for batch_idx in range(len(train_labels)):
    data, target = next(train_loader)
    # print(batch_idx)
    optimizer.zero_grad()
    # Forward pass
    output = network(data)
    # Calculating loss
    loss = loss_func(output, target)
    # Backward pass
    loss.backward()
    # Optimize the weights
    optimizer.step()

    total_loss.append(loss.item())

    print('\r Epoch %d ~ Batch %d (%d) ~ Loss %f ' % (epoch, batch_idx, len(train_imgs)-1, loss.item()), end='\t\t')


  with torch.no_grad():
    val_loss = []
    targets = []
    predictions = []
    for batch_idx in range(len(val_images)):
      data, target = next(test_loader)
      output = network(data)
      loss = loss_func(output, target)
      val_loss.append(loss.item())

      targets.append(target.item())

      predictions.append(network.predict(data).item())


  train_loss_list.append(sum(total_loss)/len(total_loss))
  val_loss_list.append(sum(val_loss)/len(val_loss))

  print('Training [{:.0f}%]\t Training Loss: {:.4f} Validation Loss: {:.4f}'.format(
      100. * (epoch + 1) / epochs, train_loss_list[-1], val_loss_list[-1]))

  if epoch % 3 == 1:
    print(confusion_matrix(targets, predictions,normalize='true'))
    print(classification_report(targets, predictions, target_names=classes, digits=4))
    torch.save({
            'epoch': epoch,
            'model_state_dict': network.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': train_loss_list[-1],
            }, '/content/EuroSAT')

from qiskit_aer import AerSimulator as Aer
from qiskit_ibm_runtime import QiskitRuntimeService, SamplerV2 as Sampler
import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

# Set up the backend for quantum simulation
aer_sim = AerSimulator()

train_loss_list = []
val_loss_list = []
epochs = 10
loss_func = nn.CrossEntropyLoss()

for epoch in range(epochs):
    train_loader = iter(handler.qcnn_data_loader(train_imgs, train_labels, batch_size=1, img_shape=(64,64,3)))
    test_loader = iter(handler.qcnn_data_loader(val_images, val_labels, batch_size=1, img_shape=(64,64,3)))
    total_loss = []

    for batch_idx in range(len(train_labels)):
        data, target = next(train_loader)

        optimizer.zero_grad()

        # Forward pass through the classical model and the quantum model
        classical_output = network(data)  # Forward pass through the network
        quantum_output = get_quantum_model_output(data)  # Quantum output

        # Ensure quantum_output matches the shape of the classical output (batch size, num_classes)
        quantum_output = quantum_output.unsqueeze(0)  # Add batch dimension if needed
        output = classical_output + quantum_output  # Hybrid model (NN + Quantum)

        # Calculating loss
        loss = loss_func(output, target)

        # Backward pass
        loss.backward()

        # Optimize the weights
        optimizer.step()

        total_loss.append(loss.item())

        print(f'\r Epoch {epoch} ~ Batch {batch_idx}/{len(train_imgs)-1} ~ Loss {loss.item():.6f}', end='\t\t')

    # Validation loop
    with torch.no_grad():
        val_loss = []
        targets = []
        predictions = []

        for batch_idx in range(len(val_images)):
            data, target = next(test_loader)

            # Move data and target to the same device as the model (CUDA or CPU)
            data, target = data.to(device), target.to(device)

            # Forward pass through the network
            output = network(data)

            # Calculating loss
            loss = loss_func(output, target)
            val_loss.append(loss.item())

            targets.append(target.item())
            predictions.append(network.predict(data).item())

    # Calculate and store average losses
    train_loss_list.append(np.mean(total_loss))
    val_loss_list.append(np.mean(val_loss))

    print(f'Training [{(100. * (epoch + 1) / epochs):.0f}%]\t Training Loss: {train_loss_list[-1]:.4f} Validation Loss: {val_loss_list[-1]:.4f}')

    # Print confusion matrix and classification report periodically
    if epoch % 3 == 1:
        print(confusion_matrix(targets, predictions, normalize='true'))
        print(classification_report(targets, predictions, target_names=classes, digits=4))

        # Save the model checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': network.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': train_loss_list[-1],
        }, '/content/EuroSAT')

from qiskit_aer import AerSimulator
from qiskit import QuantumCircuit
from qiskit_ibm_runtime import QiskitRuntimeService, SamplerV2 as Sampler
import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

# Set up the backend for quantum simulation
aer_sim = AerSimulator()

train_loss_list = []
val_loss_list = []
epochs = 10
loss_func = nn.CrossEntropyLoss()

for epoch in range(epochs):
    train_loader = iter(handler.qcnn_data_loader(train_imgs, train_labels, batch_size=1, img_shape=(64,64,3)))
    test_loader = iter(handler.qcnn_data_loader(val_images, val_labels, batch_size=1, img_shape=(64,64,3)))
    total_loss = []

    for batch_idx in range(len(train_labels)):
        data, target = next(train_loader)

        optimizer.zero_grad()

        # Forward pass through the classical model and the quantum model
        classical_output = network(data)  # Forward pass through the network
        quantum_output = get_quantum_model_output(data)  # Quantum output

        # Ensure quantum_output matches the shape of the classical output (batch size, num_classes)
        quantum_output = quantum_output.unsqueeze(0)  # Add batch dimension if needed
        output = classical_output + quantum_output  # Hybrid model (NN + Quantum)

        # Calculating loss
        loss = loss_func(output, target)

        # Backward pass
        loss.backward()

        # Optimize the weights
        optimizer.step()

        total_loss.append(loss.item())

        print(f'\r Epoch {epoch} ~ Batch {batch_idx}/{len(train_imgs)-1} ~ Loss {loss.item():.6f}', end='\t\t')

    # Validation loop
    with torch.no_grad():
        val_loss = []
        targets = []
        predictions = []

        for batch_idx in range(len(val_images)):
            data, target = next(test_loader)

            # Move data and target to the same device as the model (CUDA or CPU)
            data, target = data.to(device), target.to(device)

            # Forward pass through the network
            output = network(data)

            # Calculating loss
            loss = loss_func(output, target)
            val_loss.append(loss.item())

            targets.append(target.item())
            predictions.append(network.predict(data).item())

    # Calculate and store average losses
    train_loss_list.append(np.mean(total_loss))
    val_loss_list.append(np.mean(val_loss))

    print(f'Training [{(100. * (epoch + 1) / epochs):.0f}%]\t Training Loss: {train_loss_list[-1]:.4f} Validation Loss: {val_loss_list[-1]:.4f}')

    # Print confusion matrix and classification report periodically
    if epoch % 3 == 1:
        print(confusion_matrix(targets, predictions, normalize='true'))
        print(classification_report(targets, predictions, target_names=classes, digits=4))

        # Save the model checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': network.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': train_loss_list[-1],
        }, '/content/EuroSAT')

from qiskit_aer import AerSimulator
backend = AerSimulator()
from qiskit import QuantumCircuit
from qiskit_ibm_runtime import QiskitRuntimeService, SamplerV2 as Sampler
import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

# Set up the backend for quantum simulation
# aer_sim = AerSimulator()  # Direct instantiation without 'get_backend'

train_loss_list = []
val_loss_list = []
epochs = 10
loss_func = nn.CrossEntropyLoss()

for epoch in range(epochs):
    train_loader = iter(handler.qcnn_data_loader(train_imgs, train_labels, batch_size=1, img_shape=(64,64,3)))
    test_loader = iter(handler.qcnn_data_loader(val_images, val_labels, batch_size=1, img_shape=(64,64,3)))
    total_loss = []

    for batch_idx in range(len(train_labels)):
        data, target = next(train_loader)

        optimizer.zero_grad()

        # Forward pass through the classical model and the quantum model
        classical_output = network(data)  # Forward pass through the network
        quantum_output = get_quantum_model_output(data)  # Quantum output

        # Ensure quantum_output matches the shape of the classical output (batch size, num_classes)
        quantum_output = quantum_output.unsqueeze(0)  # Add batch dimension if needed
        output = classical_output + quantum_output  # Hybrid model (NN + Quantum)

        # Calculating loss
        loss = loss_func(output, target)

        # Backward pass
        loss.backward()

        # Optimize the weights
        optimizer.step()

        total_loss.append(loss.item())

        print(f'\r Epoch {epoch} ~ Batch {batch_idx}/{len(train_imgs)-1} ~ Loss {loss.item():.6f}', end='\t\t')

    # Validation loop
    with torch.no_grad():
        val_loss = []
        targets = []
        predictions = []

        for batch_idx in range(len(val_images)):
            data, target = next(test_loader)

            # Move data and target to the same device as the model (CUDA or CPU)
            data, target = data.to(device), target.to(device)

            # Forward pass through the network
            output = network(data)

            # Calculating loss
            loss = loss_func(output, target)
            val_loss.append(loss.item())

            targets.append(target.item())
            predictions.append(network.predict(data).item())

    # Calculate and store average losses
    train_loss_list.append(np.mean(total_loss))
    val_loss_list.append(np.mean(val_loss))

    print(f'Training [{(100. * (epoch + 1) / epochs):.0f}%]\t Training Loss: {train_loss_list[-1]:.4f} Validation Loss: {val_loss_list[-1]:.4f}')

    # Print confusion matrix and classification report periodically
    if epoch % 3 == 1:
        print(confusion_matrix(targets, predictions, normalize='true'))
        print(classification_report(targets, predictions, target_names=classes, digits=4))

        # Save the model checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': network.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': train_loss_list[-1],
        }, '/content/EuroSAT')

